{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Use dataset uploaded to Google Drive"
      ],
      "metadata": {
        "id": "L7KNK_LHwYOM"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FBlu6d1epLmv"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Check if dataset is correctly set up"
      ],
      "metadata": {
        "id": "DCXWdmPlomv-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "pathToDirectory='gdrive/MyDrive/DLB/genres/'\n",
        "genres = [a for a in os.listdir(pathToDirectory) if '.' not in a]\n",
        "print(genres)"
      ],
      "metadata": {
        "id": "81bUBJ_GqOjS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Function to generate spectrograms, which we will use as an input to train our model 80 images for train, 20 images for test"
      ],
      "metadata": {
        "id": "p6az5yXnosFw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import librosa\n",
        "import matplotlib.pyplot as plt\n",
        "import librosa.display\n",
        "\n",
        "def generateSpectrogram(genre):\n",
        "  imageNames = os.listdir(pathToDirectory+genre)\n",
        "  if os.path.exists('spectrogram/train/'+genre)==False:\n",
        "    os.makedirs('spectrogram/train/'+genre)\n",
        "  if os.path.exists('spectrogram/test/'+genre)==False:\n",
        "    os.makedirs('spectrogram/test/'+genre)\n",
        "  print(genre)\n",
        "  trainNames = imageNames[:80]\n",
        "  testNames = imageNames[80:]\n",
        "  fileCounter = 1\n",
        "  for fileName in trainNames:\n",
        "    audioFile , samplingRate = librosa.load(pathToDirectory+genre+'/'+fileName)\n",
        "    audioFileFourier = librosa.stft(audioFile)\n",
        "    audioFileDB = librosa.amplitude_to_db(abs(audioFileFourier))\n",
        "    librosa.display.specshow(audioFileDB)\n",
        "    plt.savefig('spectrogram/train/'+genre+'/'+str(fileCounter)+'.png')\n",
        "    plt.close()\n",
        "    fileCounter+=1  \n",
        "  fileCounter = 1\n",
        "  for fileName in testNames:\n",
        "    fileCounter+=1\n",
        "    audioFile , samplingRate = librosa.load(pathToDirectory+genre+'/'+fileName)\n",
        "    audioFileFourier = librosa.stft(audioFile)\n",
        "    audioFileDB = librosa.amplitude_to_db(abs(audioFileFourier))\n",
        "    librosa.display.specshow(audioFileDB)\n",
        "    plt.savefig('spectrogram/test/'+genre+'/'+str(fileCounter)+'.png')\n",
        "    plt.close()\n",
        "    fileCounter+=1"
      ],
      "metadata": {
        "id": "a1ed9EWmqxh-"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Generate spectrogram for every genre"
      ],
      "metadata": {
        "id": "NhAMB4BPo4BG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for genre in genres:\n",
        "  generateSpectrogram(genre)"
      ],
      "metadata": {
        "id": "4IuMfJyk1Pf7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Imports"
      ],
      "metadata": {
        "id": "p-CCHC2opBx3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import seaborn as sns\n",
        "\n",
        "import keras\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Conv2D , MaxPool2D , Flatten , Dropout \n",
        "from keras.optimizers import Adam\n",
        "\n",
        "from sklearn.metrics import classification_report,confusion_matrix\n",
        "\n",
        "import tensorflow as tf\n",
        "\n",
        "import cv2\n",
        "import os\n",
        "\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "4XWnYlpVtT2O"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Function to get Data from dataset to an array"
      ],
      "metadata": {
        "id": "OJVHumJMpEah"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "imageSize = 256\n",
        "def getData(dataDirectory):\n",
        "    data = [] \n",
        "    for genre in genres: \n",
        "        path = os.path.join(dataDirectory, genre)\n",
        "        genreIndex = genres.index(genre)\n",
        "        images = [image for image in os.listdir(path) if '.ipynb_checkpoints' not in image]\n",
        "        for image in images:\n",
        "          imageArray = cv2.imread(os.path.join(path, image))[...,::-1] \n",
        "          resizedImageArray = cv2.resize(imageArray, (imageSize, imageSize)) # Reshape images to normalized size\n",
        "          data.append([resizedImageArray, genreIndex])\n",
        "    return np.array(data)"
      ],
      "metadata": {
        "id": "pGo2PC8qtUtv"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "call the function for Train and Validation sets"
      ],
      "metadata": {
        "id": "3mqy8pk3pQoK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "trainingSet = getData('spectrogram/train')\n",
        "validationSet = getData('spectrogram/test')"
      ],
      "metadata": {
        "id": "2yjWf_PZtyMo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "xTraining = []\n",
        "yTraining = []\n",
        "xValidation = []\n",
        "yValidation = []\n",
        "\n",
        "for feature, label in trainingSet:\n",
        "  xTraining.append(feature)\n",
        "  yTraining.append(label)\n",
        "\n",
        "for feature, label in validationSet:\n",
        "  xValidation.append(feature)\n",
        "  yValidation.append(label)\n",
        "\n",
        "xTraining = np.array(xTraining) / 255\n",
        "xValidation = np.array(xValidation) / 255\n",
        "\n",
        "xTraining.reshape(-1, imageSize, imageSize, 1)\n",
        "yTraining = np.array(yTraining)\n",
        "\n",
        "xValidation.reshape(-1, imageSize, imageSize, 1)\n",
        "yValidation = np.array(yValidation)"
      ],
      "metadata": {
        "id": "j6v7BhuzuFVL"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Based on the lectures we decided that it would be a good structure of our network, convulotional layers, pooling layers, dropout and dense layers"
      ],
      "metadata": {
        "id": "1hSrc2xSpvQz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = Sequential()\n",
        "model.add(Conv2D(32,3,padding=\"same\", activation=\"relu\", input_shape=(256,256,3)))\n",
        "model.add(MaxPool2D())\n",
        "model.add(Conv2D(32, 3, padding=\"same\", activation=\"relu\"))\n",
        "model.add(MaxPool2D())\n",
        "model.add(Conv2D(64, 3, padding=\"same\", activation=\"relu\"))\n",
        "model.add(MaxPool2D())\n",
        "model.add(Dropout(0.4))\n",
        "model.add(Flatten())\n",
        "model.add(Dense(128,activation=\"relu\"))\n",
        "model.add(Dense(10, activation=\"softmax\"))\n",
        "\n",
        "model.summary()"
      ],
      "metadata": {
        "id": "sr02CpS8urI_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We chose Adam as an optimizer with learning rate of 0.001, we might change that value in the future"
      ],
      "metadata": {
        "id": "gDbeNmCMraM0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "opt = Adam(learning_rate=0.001)\n",
        "model.compile(optimizer = opt , loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True) , metrics = ['accuracy'])"
      ],
      "metadata": {
        "id": "xs4AigLNu103"
      },
      "execution_count": 72,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%load_ext tensorboard"
      ],
      "metadata": {
        "id": "gACuYgBwKH9O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Setting up tensorboard"
      ],
      "metadata": {
        "id": "OYoAS6Vsr-23"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%reload_ext tensorboard\n",
        "\n",
        "logFolder = 'logs'\n",
        "\n",
        "import datetime\n",
        "logFolder = \"logs/fit/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
        "\n",
        "from tensorflow.keras.callbacks import TensorBoard\n",
        "\n",
        "callbacks = [TensorBoard(log_dir=logFolder,\n",
        "                         histogram_freq=1,\n",
        "                         write_graph=True,\n",
        "                         write_images=True,\n",
        "                         update_freq='epoch',\n",
        "                         profile_batch=2,\n",
        "                         embeddings_freq=1)]"
      ],
      "metadata": {
        "id": "6MCQN7eEKFjp"
      },
      "execution_count": 74,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "First run training to check if training is working correctly, 50 epochs"
      ],
      "metadata": {
        "id": "J85uA-o2sCW6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "history = model.fit(xTraining,yTraining,epochs = 50, validation_data = (xValidation, yValidation), callbacks=callbacks)\n"
      ],
      "metadata": {
        "id": "RTBRDq6Ku34y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "model.save_weights('50_epoch.cpkt')\n",
        "\n",
        "pickle.dump(history.history, open('history_50_epoch.pkl','wb'))"
      ],
      "metadata": {
        "id": "Q_UFPEOru47a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Plot training and validation accuracy"
      ],
      "metadata": {
        "id": "akvy0IEBsrpp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "accuracy = history.history['accuracy']\n",
        "validationAccuracy = history.history['val_accuracy']\n",
        "loss = history.history['loss']\n",
        "validationLoss = history.history['val_loss']\n",
        "\n",
        "epochsRange = range(50)\n",
        "\n",
        "plt.figure(figsize=(25, 15))\n",
        "plt.subplot(2, 2, 1)\n",
        "plt.plot(epochsRange, accuracy, label='Training Accuracy')\n",
        "plt.plot(epochsRange, validationAccuracy, label='Validation Accuracy')\n",
        "plt.legend(loc='lower right')\n",
        "plt.title('Training and Validation Accuracy')\n",
        "\n",
        "plt.subplot(2, 2, 2)\n",
        "plt.plot(epochsRange, loss, label='Training Loss')\n",
        "plt.plot(epochsRange, validationLoss, label='Validation Loss')\n",
        "plt.legend(loc='upper right')\n",
        "plt.title('Training and Validation Loss')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "Nv1UZRnIu65k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Plot the accuracy and loss"
      ],
      "metadata": {
        "id": "QFb7XoJqynCc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.metrics.metrics import Accuracy\n",
        "import pickle\n",
        "history = pickle.load(open('history_50_epoch.pkl','rb'))\n",
        "accuracy = history['accuracy']\n",
        "validationAccuracy = history['validationAccuracy']\n",
        "loss = history['loss']\n",
        "validationLoss = history['validationLoss']\n",
        "\n",
        "epochsRange = range(50)\n",
        "fig, (ax1, ax2) = plt.subplots(1,2, figsize=(15,6))\n",
        "plt.rc('xtick', labelsize=10)\n",
        "plt.rc('ytick', labelsize=10)\n",
        "ax1.plot(epochsRange, accuracy, label='Training Accuracy', c = 'green', linewidth=4)\n",
        "ax1.plot(epochsRange, validationAccuracy, label='Validation Accuracy', c='red', linewidth=4)\n",
        "ax1.legend()\n",
        "ax1.set_title('Training and Validation Accuracy',fontsize=18)\n",
        "ax1.set_ylabel('Accuracy',fontsize=18)\n",
        "ax1.set_xlabel('Epoch',fontsize=18)\n",
        "\n",
        "ax2.plot(epochsRange, loss, label='Training Loss',c = 'green', linewidth=4)\n",
        "ax2.plot(epochsRange, validationLoss, label='Validation Loss', c='red', linewidth=4)\n",
        "ax2.legend()\n",
        "ax2.set_title('Training and Validation Loss',fontsize=18)\n",
        "ax2.set_ylabel('Loss',fontsize=18)\n",
        "ax2.set_xlabel('Epoch',fontsize=18)\n",
        "fig.tight_layout(pad=3.0)\n",
        "plt.savefig('plot.png',bbox_inches = 'tight')\n",
        "plt.clf()\n",
        "     "
      ],
      "metadata": {
        "id": "5PrNDJ5zu8RJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Predict and generate classification report"
      ],
      "metadata": {
        "id": "ekqmnHmvy9R8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "predictions = np.argmax(model.predict(xValidation), axis=-1)\n",
        "predictions = predictions.reshape(1,-1)[0]\n",
        "print(classification_report(yValidation, predictions, target_names = genres))"
      ],
      "metadata": {
        "id": "6KZlls4Hu9Oe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Craete the confusion matrix"
      ],
      "metadata": {
        "id": "Jzend_hNzDwG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import seaborn as sn\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "cm = confusion_matrix(yValidation, predictions)\n",
        "df_cm = pd.DataFrame(cm, index = [i for i in genres],\n",
        "              columns = [i for i in genres])\n",
        "plt.figure(figsize = (10,7))\n",
        "sn.heatmap(df_cm, annot=True,cmap=\"RdPu\")\n",
        "plt.savefig('confusion_matrix.png',bbox_inches = 'tight')"
      ],
      "metadata": {
        "id": "Zcu63xQxu-ND"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%tensorboard --logdir=logs/fit"
      ],
      "metadata": {
        "id": "JlryKHnUQsDk"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}